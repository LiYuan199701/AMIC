@article{DBLP:journals/corr/cs-LG-0212032,
  author       = {Peter D. Turney},
  title        = {Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised
                  Classification of Reviews},
  journal      = {CoRR},
  volume       = {cs.LG/0212032},
  year         = {2002},
  url          = {http://arxiv.org/abs/cs/0212032},
  timestamp    = {Fri, 10 Jan 2020 12:59:09 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/cs-LG-0212032.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@phdthesis{chenyu,
  author  = {Chenyu Yang},
  title   = {INTERPRETABLE SENTIMENT ANALYSIS USING THE ATTENTION-BASED MULTIPLE INSTANCE CLASSIFICATION MODEL},
  school  = {Southern Methodist University},
  address = {Dallas, TX},
  year    = {2023},
  month   = {December}
}

@article{pitts,
	abstract = {Because of the ``all-or-none''character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	author = {McCulloch, Warren S. and Pitts, Walter},
	date = {1943/12/01},
	date-added = {2024-01-05 23:21:14 -0600},
	date-modified = {2024-01-05 23:21:14 -0600},
	doi = {10.1007/BF02478259},
	isbn = {1522-9602},
	journal = {The bulletin of mathematical biophysics},
	number = {4},
	pages = {115--133},
	title = {A logical calculus of the ideas immanent in nervous activity},
	url = {https://doi.org/10.1007/BF02478259},
	volume = {5},
	year = {1943},
	bdsk-url-1 = {https://doi.org/10.1007/BF02478259}}

@inproceedings{NIPS2012_c399862d,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}

@article{DBLP:journals/corr/abs-1910-03771,
  author       = {Thomas Wolf and
                  Lysandre Debut and
                  Victor Sanh and
                  Julien Chaumond and
                  Clement Delangue and
                  Anthony Moi and
                  Pierric Cistac and
                  Tim Rault and
                  R{\'{e}}mi Louf and
                  Morgan Funtowicz and
                  Jamie Brew},
  title        = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  journal      = {CoRR},
  volume       = {abs/1910.03771},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.03771},
  eprinttype    = {arXiv},
  eprint       = {1910.03771},
  timestamp    = {Tue, 02 Jun 2020 12:49:01 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-03771.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/VaswaniSPUJGKP17,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  title        = {Attention Is All You Need},
  journal      = {CoRR},
  volume       = {abs/1706.03762},
  year         = {2017},
  url          = {http://arxiv.org/abs/1706.03762},
  eprinttype    = {arXiv},
  eprint       = {1706.03762},
  timestamp    = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1810-04805,
  author       = {Jacob Devlin and
                  Ming{-}Wei Chang and
                  Kenton Lee and
                  Kristina Toutanova},
  title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                  Understanding},
  journal      = {CoRR},
  volume       = {abs/1810.04805},
  year         = {2018},
  url          = {http://arxiv.org/abs/1810.04805},
  eprinttype    = {arXiv},
  eprint       = {1810.04805},
  timestamp    = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@techreport{f1,
  title       = {Understanding Natural
Language Understanding},
  author      = {MacCartney, Bill},
  institution = {Stanford University},
  address     = {Stanford, CA},
  year        = {2014},
  month       = {July}
}

@article{10.1257/jel.20181020,
Author = {Gentzkow, Matthew and Kelly, Bryan and Taddy, Matt},
Title = {Text as Data},
Journal = {Journal of Economic Literature},
Volume = {57},
Number = {3},
Year = {2019},
Month = {September},
Pages = {535-74},
DOI = {10.1257/jel.20181020},
URL = {https://www.aeaweb.org/articles?id=10.1257/jel.20181020}}

@article{DBLP:journals/corr/abs-1909-12434,
  author       = {Divyansh Kaushik and
                  Eduard H. Hovy and
                  Zachary C. Lipton},
  title        = {Learning the Difference that Makes a Difference with Counterfactually-Augmented
                  Data},
  journal      = {CoRR},
  volume       = {abs/1909.12434},
  year         = {2019},
  url          = {http://arxiv.org/abs/1909.12434},
  eprinttype    = {arXiv},
  eprint       = {1909.12434},
  timestamp    = {Mon, 06 Jan 2020 07:44:53 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1909-12434.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@book{manning99foundations,
  added-at = {2007-12-07T10:02:43.000+0100},
  address = {Cambridge, Massachusetts},
  author = {Manning, Christopher D. and Sch{\"u}tze, Hinrich},
  biburl = {https://www.bibsonomy.org/bibtex/22fad675aa6ae88082af2507c16d54343/hotho},
  interhash = {a81df02f92f266a51183fe936f588a08},
  intrahash = {2fad675aa6ae88082af2507c16d54343},
  keywords = {lecture nlp},
  publisher = {The {MIT} Press},
  timestamp = {2007-12-07T10:02:43.000+0100},
  title = {Foundations of Statistical Natural Language Processing},
  url = {http://nlp.stanford.edu/fsnlp/},
  year = 1999
}

@misc{stop,
  title        = "Ranks NL",
  author       = {Damian Doyle},
  howpublished = "\url{https://www.ranks.nl/stopwords}"
}


@article{tfidf,
	abstract = {The exhaustivity of document descriptions and the specificity of index terms are usually regarded as independent. It is suggested that specificity should be interpreted statistically, as a function of term use rather than of term meaning. The effects on retrieval of variations in term specificity are examined, experiments with three test collections showing in particular that frequently‐occurring terms are required for good overall performance. It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms. Results for the test collections show that considerable improvements in performance are obtained with this very simple procedure.},
	author = {SPARCK JONES, KAREN},
	date-added = {2024-01-07 16:32:04 -0600},
	date-modified = {2024-01-07 16:32:04 -0600},
	doi = {10.1108/eb026526},
	isbn = {0022-0418},
	journal = {Journal of Documentation},
	month = {2024/01/07},
	number = {1},
	pages = {11--21},
	publisher = {MCB UP Ltd},
	title = {A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL},
	url = {https://doi.org/10.1108/eb026526},
	volume = {28},
	year = {1972},
	year1 = {1972/01/01},
	bdsk-url-1 = {https://doi.org/10.1108/eb026526}}

@Inbook{Hiemstra2009,
author="Hiemstra, Djoerd",
editor="LIU, LING
and {\"O}ZSU, M. TAMER",
title="N-Gram Models",
bookTitle="Encyclopedia of Database Systems",
year="2009",
publisher="Springer US",
address="Boston, MA",
pages="1910--1910",
isbn="978-0-387-39940-9",
doi="10.1007/978-0-387-39940-9_935",
url="https://doi.org/10.1007/978-0-387-39940-9_935"
}

@article{DBLP:journals/corr/MikolovSCCD13,
  author       = {Tom{\'{a}}s Mikolov and
                  Ilya Sutskever and
                  Kai Chen and
                  Greg Corrado and
                  Jeffrey Dean},
  title        = {Distributed Representations of Words and Phrases and their Compositionality},
  journal      = {CoRR},
  volume       = {abs/1310.4546},
  year         = {2013},
  url          = {http://arxiv.org/abs/1310.4546},
  eprinttype    = {arXiv},
  eprint       = {1310.4546},
  timestamp    = {Mon, 28 Dec 2020 11:31:02 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/MikolovSCCD13.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{pennington-etal-2014-glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = {oct},
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}


@inbook{dic,
	abstract = {A growing body of research outlines how to best facilitate and ensure methodological rigor when using dictionary-based computerized text analyses (DBCTA) in organizational research. However, these best practices are currently scattered across several methodological and empirical manuscripts, making it difficult for scholars new to the technique to implement DBCTA in their own research. To better equip researchers looking to leverage this technique, this methodological report consolidates current best practices for applying DBCTA into a single, practical guide. In doing so, we provide direction regarding how to make key design decisions and identify valuable resources to help researchers from the beginning of the research process through final publication. Consequently, we advance DBCTA methods research by providing a one-stop reference for novices and experts alike concerning current best practices and available resources.},
	author = {Reid, Shane W. and McKenny, Aaron F. and Short, Jeremy C.},
	booktitle = {Methods to Improve Our Field},
	date-added = {2024-01-08 22:12:54 -0600},
	date-modified = {2024-01-08 22:12:54 -0600},
	doi = {10.1108/S1479-838720220000014004},
	editor = {Hill, Aaron D. and McKenny, Aaron F. and O'Kane, Paula and Paroutis, Sotirios},
	isbn = {978-1-80455-365-7, 978-1-80455-364-0},
	month = {2024/01/09},
	pages = {43--78},
	publisher = {Emerald Publishing Limited},
	series = {Research Methodology in Strategy and Management},
	title = {Synthesizing Best Practices for Conducting Dictionary-Based Computerized Text Analysis Research},
	url = {https://doi.org/10.1108/S1479-838720220000014004},
	volume = {14},
	year = {2023},
	year1 = {2023/01/01},
	bdsk-url-1 = {https://doi.org/10.1108/S1479-838720220000014004}}

@article{a92f3c16-7c6e-31d3-b403-82d2b0a469e4,
 ISSN = {00401706},
 URL = {http://www.jstor.org/stable/1267351},
 abstract = {In multiple regression it is shown that parameter estimates based on minimum residual sum of squares have a high probability of being unsatisfactory, if not incorrect, if the prediction vectors are not orthogonal. Proposed is an estimation procedure based on adding small positive quantities to the diagonal of X′X. Introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality. It is then shown how to augment X′X to obtain biased estimates with smaller mean square error.},
 author = {Arthur E. Hoerl and Robert W. Kennard},
 journal = {Technometrics},
 number = {1},
 pages = {55--67},
 publisher = {[Taylor & Francis, Ltd., American Statistical Association, American Society for Quality]},
 title = {Ridge Regression: Biased Estimation for Nonorthogonal Problems},
 urldate = {2024-01-09},
 volume = {12},
 year = {1970}
}

@article{51791361-8fe2-38d5-959f-ae8d048b490d,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346178},
 abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
 author = {Robert Tibshirani},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {267--288},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regression Shrinkage and Selection via the Lasso},
 urldate = {2024-01-09},
 volume = {58},
 year = {1996}
}

@article{b61d584f-9c9c-3cf8-b866-214ec2216250,
 ISSN = {13697412, 14679868},
 URL = {http://www.jstor.org/stable/3647580},
 abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p ≫ n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
 author = {Hui Zou and Trevor Hastie},
 journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
 number = {2},
 pages = {301--320},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regularization and Variable Selection via the Elastic Net},
 urldate = {2024-01-09},
 volume = {67},
 year = {2005}
}


@article{log,
	abstract = {It is now well understood that (1) it is possible to reconstruct sparse signals exactly from what appear to be highly incomplete sets of linear measurements and (2) that this can be done by constrained l1 minimization. In this paper, we study a novel method for sparse signal recovery that in many situations outperforms l1 minimization in the sense that substantially fewer measurements are needed for exact recovery. The algorithm consists of solving a sequence of weighted l1-minimization problems where the weights used for the next iteration are computed from the value of the current solution. We present a series of experiments demonstrating the remarkable performance and broad applicability of this algorithm in the areas of sparse signal recovery, statistical estimation, error correction and image processing. Interestingly, superior gains are also achieved when our method is applied to recover signals with assumed near-sparsity in overcomplete representations---not by reweighting the l1 norm of the coefficient sequence as is common, but by reweighting the l1 norm of the transformed object. An immediate consequence is the possibility of highly efficient data acquisition protocols by improving on a technique known as Compressive Sensing.},
	author = {Cand{\`e}s, Emmanuel J. and Wakin, Michael B. and Boyd, Stephen P.},
	date = {2008/12/01},
	date-added = {2024-01-09 12:35:01 -0600},
	date-modified = {2024-01-09 12:35:01 -0600},
	doi = {10.1007/s00041-008-9045-x},
	isbn = {1531-5851},
	journal = {Journal of Fourier Analysis and Applications},
	number = {5},
	pages = {877--905},
	title = {Enhancing Sparsity by Reweighted l1 Minimization},
	url = {https://doi.org/10.1007/s00041-008-9045-x},
	volume = {14},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1007/s00041-008-9045-x}}

@article{1570572700669207808,
author="HOTELLING, H.",
title="Analysis of a complex of statistical variables with principal components",
journal="J. Educ. Psy.",
year="1933",
volume="24",
pages="498-520",
URL="https://cir.nii.ac.jp/crid/1570572700669207808"
}

@inbook{pls,
author = {Wold, H.},
year = {2004},
month = {10},
pages = {},
title = {Partial Least Squares},
isbn = {0471667196},
journal = {Encyclopedia Statistical Sci},
doi = {10.1002/0471667196.ess1914}
}

@article{68aee965-a8a0-3e72-9f89-8d89ae91a62b,
 ISSN = {00359238},
 URL = {http://www.jstor.org/stable/2344614},
 abstract = {The technique of iterative weighted linear regression can be used to obtain maximum likelihood estimates of the parameters with observations distributed according to some exponential family and systematic effects that can be made linear by a suitable transformation. A generalization of the analysis of variance is given for these models using log-likelihoods. These generalized linear models are illustrated by examples relating to four distributions; the Normal, Binomial (probit analysis, etc.), Poisson (contingency tables) and gamma (variance components). The implications of the approach in designing statistics courses are discussed.},
 author = {J. A. Nelder and R. W. M. Wedderburn},
 journal = {Journal of the Royal Statistical Society. Series A (General)},
 number = {3},
 pages = {370--384},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Generalized Linear Models},
 urldate = {2024-01-09},
 volume = {135},
 year = {1972}
}

@inproceedings{boser1992training,
  title={A training algorithm for optimal margin classifiers},
  author={Boser, Bernhard E and Guyon, Isabelle M and Vapnik, Vladimir N},
  booktitle={Proceedings of the fifth annual workshop on Computational learning theory},
  pages={144--152},
  year={1992}
}


@article{rf,
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	author = {Breiman, Leo},
	date = {2001/10/01},
	date-added = {2024-01-09 20:14:01 -0600},
	date-modified = {2024-01-09 20:14:01 -0600},
	doi = {10.1023/A:1010933404324},
	isbn = {1573-0565},
	journal = {Machine Learning},
	number = {1},
	pages = {5--32},
	title = {Random Forests},
	url = {https://doi.org/10.1023/A:1010933404324},
	volume = {45},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1023/A:1010933404324}}

@article{DBLP:journals/corr/ChenG16,
  author       = {Tianqi Chen and
                  Carlos Guestrin},
  title        = {XGBoost: {A} Scalable Tree Boosting System},
  journal      = {CoRR},
  volume       = {abs/1603.02754},
  year         = {2016},
  url          = {http://arxiv.org/abs/1603.02754},
  eprinttype    = {arXiv},
  eprint       = {1603.02754},
  timestamp    = {Sat, 17 Dec 2022 01:15:28 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/ChenG16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{bengio2000neural,
  title={A neural probabilistic language model},
  author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal},
  journal={Advances in neural information processing systems},
  volume={13},
  year={2000}
}

@article{doi:10.1080/01621459.2012.734168,
author = {Matt Taddy},
title = {Multinomial Inverse Regression for Text Analysis},
journal = {Journal of the American Statistical Association},
volume = {108},
number = {503},
pages = {755-770},
year = {2013},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2012.734168},


URL = { 
    
        https://doi.org/10.1080/01621459.2012.734168
    
    

},
eprint = { 
    
        https://doi.org/10.1080/01621459.2012.734168
}
}


@article{appro,
	abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	author = {Cybenko, G. },
	date = {1989/12/01},
	date-added = {2024-01-10 11:56:36 -0600},
	date-modified = {2024-01-10 11:56:36 -0600},
	doi = {10.1007/BF02551274},
	isbn = {1435-568X},
	journal = {Mathematics of Control, Signals and Systems},
	number = {4},
	pages = {303--314},
	title = {Approximation by superpositions of a sigmoidal function},
	url = {https://doi.org/10.1007/BF02551274},
	volume = {2},
	year = {1989},
	bdsk-url-1 = {https://doi.org/10.1007/BF02551274}}

@article{DBLP:journals/corr/Kim14f,
  author       = {Yoon Kim},
  title        = {Convolutional Neural Networks for Sentence Classification},
  journal      = {CoRR},
  volume       = {abs/1408.5882},
  year         = {2014},
  url          = {http://arxiv.org/abs/1408.5882},
  eprinttype    = {arXiv},
  eprint       = {1408.5882},
  timestamp    = {Mon, 13 Aug 2018 16:46:21 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/Kim14f.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{10.1162/neco.1997.9.8.1735,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long Short-Term Memory},
year = {1997},
issue_date = {November 15, 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
doi = {10.1162/neco.1997.9.8.1735},
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
journal = {Neural Comput.},
month = {nov},
pages = {1735–1780},
numpages = {46}
}

@article{DBLP:journals/corr/LiuSLW16,
  author       = {Yang Liu and
                  Chengjie Sun and
                  Lei Lin and
                  Xiaolong Wang},
  title        = {Learning Natural Language Inference using Bidirectional {LSTM} model
                  and Inner-Attention},
  journal      = {CoRR},
  volume       = {abs/1605.09090},
  year         = {2016},
  url          = {http://arxiv.org/abs/1605.09090},
  eprinttype    = {arXiv},
  eprint       = {1605.09090},
  timestamp    = {Wed, 28 Nov 2018 09:26:28 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/LiuSLW16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1801-07883,
  author       = {Lei Zhang and
                  Shuai Wang and
                  Bing Liu},
  title        = {Deep Learning for Sentiment Analysis : {A} Survey},
  journal      = {CoRR},
  volume       = {abs/1801.07883},
  year         = {2018},
  url          = {http://arxiv.org/abs/1801.07883},
  eprinttype    = {arXiv},
  eprint       = {1801.07883},
  timestamp    = {Thu, 24 Sep 2020 18:35:11 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1801-07883.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@PHDTHESIS{Murdoch2019-uy,
  title    = "Interpretable deep learning for natural language processing",
  author   = "Murdoch, William James",
  abstract = "Author(s): Murdoch, William James | Advisor(s): Yu, Bin |
              Abstract: Machine-learning models have demonstrated great success
              in learning complex patterns that enable them to make predictions
              about unobserved data. In addition to using models for
              prediction, the ability to interpret what a model has learned is
              receiving an increasing amount of attention. These
              interpretations have found a number of uses, ranging from
              providing scientific insight to auditing the predictions
              themselves to ensure fairness with respect to protected
              categories like race or gender. However, there is still
              considerable confusion about the notion of interpretability. In
              particular, it is currently unclear both what it means to
              interpret a ML model, and how to actually do so.In the first part
              of this thesis, we address the foundational question of what it
              means to interpret a ML model. In particular, it is currently
              unclear what it means to be interpretable, and how to select,
              evaluate, or even discuss, methods for producing interpretations
              of machine-learning models. We aim to clarify these concerns by
              defining interpretable machine learning and constructing a
              unifying framework for existing methods which highlights the
              under-appreciated role played by human audiences. Within this
              framework, methods are organized into two classes: model-based
              and post-hoc. To provide guidance in selecting and evaluating
              interpretation methods, we introduce three desiderata: predictive
              accuracy, descriptive accuracy, and relevancy. Using our
              framework, we review existing work, grounded in real-world
              studies which exemplify our desiderata, and suggest directions
              for future work.The second through fourth parts introduce a
              succession of methods for interpreting predictions made by neural
              networks. The second part focuses on Long Short Term Memory
              networks (LSTMs) trained on question-answering and sentiment
              analysis, two popular tasks in natural language processing. By
              decomposing the LSTM's update equations, we introduce a novel
              method for computing feature importance scores of specific inputs
              for determining the output of an LSTM. In order to verify the
              output of our method, we use the introduced scores to search for
              consistently important patterns of words learned by state ofthe
              art LSTMs on sentiment analysis and question answering. This
              representation is then quantitatively validated by using the
              extracted phrases to construct a simple, rule-based classifier
              which approximates the output of the LSTM.While feature
              importance scores are helpful in understanding a model's
              predictions, they ignore the complex interactions between
              variables typically learned by neural networks. To this end, the
              third part introduces contextual decomposition (CD), an
              interpretation algorithm for analysing individual predictions
              made by standard LSTMs, without any changes to the underlying
              model. By decomposing the output of a LSTM, CD captures the
              contributions of combinations of words or variables to the final
              prediction of an LSTM. On the task of sentiment analysis with the
              Yelp and Stanford Sentiment Treebank (SST) data sets, we show
              that CD is able to reliably identify words and phrases of
              contrasting sentiment, and how they are combined to yield the
              LSTM's final prediction. Using the phrase-level labels in SST, we
              also demonstrate that CD is able to successfully extract positive
              and negative negations from an LSTM, something which has not
              previously been done.When considering interactions between
              variables, the number of interactions quickly becomes too large
              for manual inspection, leading to the question of how to
              automatically select and display an informative subset. In the
              fourth part, we introduce the use of hierarchical interpretations
              to explain DNN predictions through our proposed method:
              agglomerative contextual decomposition (ACD). Given a prediction
              from a trained DNN, ACD produces a hierarchical clustering of the
              input features, along with the contribution of each cluster to
              the final prediction. This hierarchy is optimized to identify
              clusters of features that the DNN learned are predictive. We
              introduce ACD using examples from Stanford Sentiment Treebank and
              ImageNet, in order to diagnose incorrect predictions, identify
              dataset bias, and extract polarizing phrases of varying lengths.
              Through human experiments, we demonstrate that ACD enables users
              both to identify the more accurate of two DNNs and to better
              trust a DNN's outputs. We also find that ACD's hierarchy is
              largely robust to adversarial perturbations, implying that it
              captures fundamental aspects of the input and ignores spurious
              noise.",
  year     =  2019,
  school   = "UC Berkeley"
}

@PHDTHESIS{Xiong2021-zt,
  title    = "Bayesian multiple instance learning with application to cancer
              detection using {TCR} repertoire sequencing data",
  author   = "Xiong, Danyi",
  abstract = "As a branch of machine learning, multiple instance learning (MIL)
              learns from a collection of labeled bags, each containing a set
              of instances. Each instance is described by a feature vector. The
              learning process is weakly supervised due to ambiguous instance
              labels. Since its emergence, MIL has been applied to solve
              various problems including content-based image retrieval, object
              tracking/detection, and computer-aided diagnosis. In biomedical
              research, the use of MIL has been focused on medical image
              analysis and molecule activity prediction. The first part of this
              dissertation focuses on a comparative study of MIL methods for a
              novel biomedical application. To date, the majority of the
              off-the-shelf MIL methods are developed in the computer science
              domain and so algorithm-driven. We review and apply a large
              collection of existing methods to investigate the applicability
              of MIL to cancer detection using T-cell receptor (TCR) sequences.
              This important application can be a viable approach for
              large-scale cancer screening, as TCRs can be easily profiled from
              a subject's peripheral blood. Based on our numerical results from
              extensive simulation and analysis of sequencing data from The
              Cancer Genome Atlas for ten types of cancer, we make suggestions
              about selection of a proper method and avoidance of any method
              with poor performance. We further identify a pressing need of new
              model-based MIL methodologies for accurate modeling of
              increasingly complex structures of real world data and more
              explainable outcomes. The second part of this dissertation
              proposes a novel Bayesian MIL method for binary classification
              based on hierarchical probit regression (MICProB), which
              contributes a significant portion to the suite of statistical
              methodologies for MIL. MICProB is composed of two nested probit
              regression models, where the inner model is estimated for
              predicting primary instances, which are considered as the
              ``important'' ones that determine the bag label, and the outer
              model is for predicting bag labels based on the features of
              primary instances estimated by the inner model. The posterior
              distribution of MICProB can be conveniently approximated using a
              Gibbs sampler, and the prediction for new bags can be performed
              in a fully integrated Bayesian way. We evaluate the performance
              of MICProB against various benchmark methods and demonstrate its
              competitiveness in simulation and real data examples. In addition
              to its capability of identifying primary instances, as compared
              to existing optimization-based approaches, MICProB also enjoys
              great advantages in providing a transparent model structure,
              straightforward statistical inference of quantities related to
              model parameters,",
  year     =  2021,
  school   = "Southern Methodist University"
}

@article{katumullage_yang_barth_cao_2022, title={Using Neural Network Models for Wine Review Classification}, volume={17}, DOI={10.1017/jwe.2022.2}, number={1}, journal={Journal of Wine Economics}, publisher={Cambridge University Press}, author={Katumullage, Duwani and Yang, Chenyu and Barth, Jackson and Cao, Jing}, year={2022}, pages={27–41}}

@article{yang_barth_katumullage_cao_2022, title={Wine Review Descriptors as Quality Predictors: Evidence from Language Processing Techniques}, volume={17}, DOI={10.1017/jwe.2022.3}, number={1}, journal={Journal of Wine Economics}, publisher={Cambridge University Press}, author={Yang, Chenyu and Barth, Jackson and Katumullage, Duwani and Cao, Jing}, year={2022}, pages={64–80}}

@inproceedings{maas-etal-2011-learning,
    title = "Learning Word Vectors for Sentiment Analysis",
    author = "Maas, Andrew L.  and
      Daly, Raymond E.  and
      Pham, Peter T.  and
      Huang, Dan  and
      Ng, Andrew Y.  and
      Potts, Christopher",
    editor = "Lin, Dekang  and
      Matsumoto, Yuji  and
      Mihalcea, Rada",
    booktitle = "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    month = "jun",
    year = "2011",
    address = "Portland, Oregon, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P11-1015",
    pages = "142--150",
}

@inproceedings{bowman-etal-2015-large,
    title = "A large annotated corpus for learning natural language inference",
    author = "Bowman, Samuel R.  and
      Angeli, Gabor  and
      Potts, Christopher  and
      Manning, Christopher D.",
    editor = "M{\`a}rquez, Llu{\'\i}s  and
      Callison-Burch, Chris  and
      Su, Jian",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = "sep",
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1075",
    doi = "10.18653/v1/D15-1075",
    pages = "632--642",
}

@inproceedings{belz-etal-2021-systematic,
    title = "A Systematic Review of Reproducibility Research in Natural Language Processing",
    author = "Belz, Anya  and
      Agarwal, Shubham  and
      Shimorina, Anastasia  and
      Reiter, Ehud",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = "apr",
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.29",
    doi = "10.18653/v1/2021.eacl-main.29",
    pages = "381--393",
    abstract = "Against the background of what has been termed a reproducibility crisis in science, the NLP field is becoming increasingly interested in, and conscientious about, the reproducibility of its results. The past few years have seen an impressive range of new initiatives, events and active research in the area. However, the field is far from reaching a consensus about how reproducibility should be defined, measured and addressed, with diversity of views currently increasing rather than converging. With this focused contribution, we aim to provide a wide-angle, and as near as possible complete, snapshot of current work on reproducibility in NLP,",
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}