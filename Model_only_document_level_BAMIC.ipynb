{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LiYuan199701/AMIC/blob/document-level-BAMIC/Model_only_document_level_BAMIC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jmRWG9wXETkH",
      "metadata": {
        "id": "jmRWG9wXETkH"
      },
      "source": [
        "# Display Model codes for document-level BAMIC"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C5aSUqcMdnZc",
      "metadata": {
        "id": "C5aSUqcMdnZc"
      },
      "source": [
        "# Basic package loading and import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5721c64d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5721c64d",
        "outputId": "1af21b0e-e37b-41fd-e017-7e9032e1cafc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.0+cu126'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "import pyximport\n",
        "import os\n",
        "import IPython\n",
        "import Cython\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "import scipy\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import seaborn as sns\n",
        "import random, math, sys, string, re\n",
        "%matplotlib inline\n",
        "# Set inline backend and resolution\n",
        "%config InlineBackend.figure_format = 'retina'  # For Retina displays\n",
        "plt.rcParams['figure.dpi'] = 100  # Set dpi to 200 for high resolution\n",
        "\n",
        "# Import keras and tensorflow\n",
        "#from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "#from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Split dataset into train, val, test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import Torch Package and its functions\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn import Parameter\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fZYixZXu7vj7",
      "metadata": {
        "id": "fZYixZXu7vj7"
      },
      "source": [
        "# Bayesian AMIC structure"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayesian last-layer class"
      ],
      "metadata": {
        "id": "diMp7prmC8ix"
      },
      "id": "diMp7prmC8ix"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G32n3Phr7jT9",
      "metadata": {
        "id": "G32n3Phr7jT9"
      },
      "outputs": [],
      "source": [
        "#%%writefile models/models.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "import scipy\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import seaborn as sns\n",
        "import random, math, sys, string, re\n",
        "# Import Torch Package and its functions\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn import Parameter\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# =========================\n",
        "# 2) Bayesian last-layer pieces\n",
        "# =========================\n",
        "class BayesLinear(nn.Module):\n",
        "    \"\"\"Factorized Gaussian posterior; prior N(0, sigma_p^2 I).\"\"\"\n",
        "    def __init__(self, in_features, out_features, prior_sigma=1.0, init_rho=-3.0):\n",
        "        super().__init__()\n",
        "        self.weight_mu  = nn.Parameter(torch.zeros(out_features, in_features))\n",
        "        self.weight_rho = nn.Parameter(torch.full((out_features, in_features), float(init_rho)))\n",
        "        self.bias_mu    = nn.Parameter(torch.zeros(out_features))\n",
        "        self.bias_rho   = nn.Parameter(torch.full((out_features,), float(init_rho)))\n",
        "        self.register_buffer(\"prior_sigma\", torch.tensor(float(prior_sigma)))\n",
        "\n",
        "    @property\n",
        "    def weight_sigma(self):\n",
        "        return F.softplus(self.weight_rho) + 1e-8\n",
        "\n",
        "    @property\n",
        "    def bias_sigma(self):\n",
        "        return F.softplus(self.bias_rho) + 1e-8\n",
        "\n",
        "    def kl(self) -> torch.Tensor:\n",
        "        w_mu, w_sigma = self.weight_mu, self.weight_sigma\n",
        "        b_mu, b_sigma = self.bias_mu,   self.bias_sigma\n",
        "        p_sigma = self.prior_sigma\n",
        "        kl_w = torch.log(p_sigma / w_sigma) + (w_sigma**2 + w_mu**2) / (2 * p_sigma**2) - 0.5\n",
        "        kl_b = torch.log(p_sigma / b_sigma) + (b_sigma**2 + b_mu**2) / (2 * p_sigma**2) - 0.5\n",
        "        return kl_w.sum() + kl_b.sum()\n",
        "\n",
        "    def forward(self, x, sample: bool = True):\n",
        "        if self.training or sample:\n",
        "            W = self.weight_mu + self.weight_sigma * torch.randn_like(self.weight_mu)\n",
        "            b = self.bias_mu   + self.bias_sigma   * torch.randn_like(self.bias_mu)\n",
        "        else:\n",
        "            W, b = self.weight_mu, self.bias_mu\n",
        "        return F.linear(x, W, b)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create_emb_layer function"
      ],
      "metadata": {
        "id": "OwM4ecC_DATJ"
      },
      "id": "OwM4ecC_DATJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------\n",
        "# create_emb_layer\n",
        "# -------------------------------------------------------------------\n",
        "def create_emb_layer(weights_matrix, non_trainable=False):\n",
        "\n",
        "    num_embeddings, embedding_dim = pad_embedding_matrix.size()\n",
        "    emb_layer = nn.Embedding.from_pretrained(pad_embedding_matrix)\n",
        "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
        "    if non_trainable:\n",
        "        emb_layer.weight.requires_grad = False\n",
        "\n",
        "    return emb_layer, num_embeddings, embedding_dim"
      ],
      "metadata": {
        "id": "8yVpc-D7CbYW"
      },
      "id": "8yVpc-D7CbYW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self‐Attention"
      ],
      "metadata": {
        "id": "OqxTJAhjDDbH"
      },
      "id": "OqxTJAhjDDbH"
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Self‐Attention\n",
        "# -------------------------------------------------------------------\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head self-attention with standard scaling by 1/sqrt(S),\n",
        "    where S = emb // heads is the per-head dimensionality.\n",
        "\n",
        "    Args:\n",
        "        emb (int): token embedding size E.\n",
        "        heads (int): number of attention heads H.\n",
        "        mask (bool): if True, apply causal masking (no attending to future tokens).\n",
        "        reduced_dim (int|None): optional projection size after attention.\n",
        "        attn_dropout (float): dropout on attention weights.\n",
        "        proj_dropout (float): dropout on the unified head output.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb, heads=8, mask=False, reduced_dim=None,\n",
        "                 attn_dropout=0.10, proj_dropout=0.20):\n",
        "        super().__init__()\n",
        "        assert emb % heads == 0, (\n",
        "            f'Embedding dimension ({emb}) should be divisible by nr. of heads ({heads})'\n",
        "        )\n",
        "\n",
        "        self.emb   = emb\n",
        "        self.heads = heads\n",
        "        self.mask  = mask\n",
        "\n",
        "        # Linear projections to K, Q, V (E -> E), no bias (common choice)\n",
        "        self.tokeys    = nn.Linear(emb, emb, bias=False)\n",
        "        self.toqueries = nn.Linear(emb, emb, bias=False)\n",
        "        self.tovalues  = nn.Linear(emb, emb, bias=False)\n",
        "\n",
        "        # After concatenating heads back to E, mix them\n",
        "        self.unifyheads = nn.Linear(emb, emb, bias=True)\n",
        "\n",
        "        # Optional final projection E -> reduced_dim\n",
        "        self.projection = nn.Linear(emb, reduced_dim, bias=True) if reduced_dim is not None else None\n",
        "\n",
        "        # Optional dropouts\n",
        "        self.attn_drop = nn.Dropout(attn_dropout) if attn_dropout > 0 else nn.Identity()\n",
        "        self.proj_drop = nn.Dropout(proj_dropout) if proj_dropout > 0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, T, E)\n",
        "        returns: (B, T, E) or (B, T, reduced_dim) if projection is used\n",
        "        \"\"\"\n",
        "        B, T, E = x.size()\n",
        "        H = self.heads\n",
        "        assert E == self.emb, f'Input embedding dim ({E}) should match layer embedding dim ({self.emb})'\n",
        "        S = E // H  # per-head size\n",
        "\n",
        "        # Project to K, Q, V: (B,T,E)\n",
        "        K = self.tokeys(x)\n",
        "        Q = self.toqueries(x)\n",
        "        V = self.tovalues(x)\n",
        "\n",
        "        # Reshape to heads: (B,H,T,S)\n",
        "        K = K.view(B, T, H, S).transpose(1, 2).contiguous()\n",
        "        Q = Q.view(B, T, H, S).transpose(1, 2).contiguous()\n",
        "        V = V.view(B, T, H, S).transpose(1, 2).contiguous()\n",
        "\n",
        "        # Scaled dot-product attention: scores (B,H,T,T)\n",
        "        # Scale by sqrt(S) — the standard Transformer choice\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(S)\n",
        "\n",
        "        # Optional causal mask: disallow attending to future positions\n",
        "        if self.mask:\n",
        "            # mask shape (T,T), True where we want to mask (upper triangle)\n",
        "            causal_mask = torch.triu(torch.ones(T, T, device=x.device, dtype=torch.bool), diagonal=1)\n",
        "            scores = scores.masked_fill(causal_mask, float('-inf'))\n",
        "\n",
        "        # Softmax over keys dimension, then dropout on weights\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        # Weighted sum of values -> (B,H,T,S)\n",
        "        out = torch.matmul(attn, V)\n",
        "\n",
        "        # Merge heads: (B,T,E)\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, E)\n",
        "\n",
        "        # Mix heads, optional dropout\n",
        "        out = self.unifyheads(out)\n",
        "        out = self.proj_drop(out)\n",
        "\n",
        "        # Optional final projection: (B,T,reduced_dim)\n",
        "        if self.projection is not None:\n",
        "            out = self.projection(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "6DZ055hXCgS1"
      },
      "id": "6DZ055hXCgS1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tiedLinear"
      ],
      "metadata": {
        "id": "xOqbRGOsDHGK"
      },
      "id": "xOqbRGOsDHGK"
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# tiedLinear\n",
        "# ------------------------------------------------------------\n",
        "class tiedLinear(nn.Module):\n",
        "    def __init__(self, in_features, bias=True):\n",
        "        super(tiedLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.weight = Parameter(torch.Tensor(in_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(1))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(0))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input):\n",
        "        repeated_weight = self.weight.repeat(self.in_features, 1)\n",
        "        return F.linear(input, repeated_weight, self.bias)\n"
      ],
      "metadata": {
        "id": "fwMx-xLKCjcM"
      },
      "id": "fwMx-xLKCjcM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PositionalEncoding class"
      ],
      "metadata": {
        "id": "zJNJPo8aDKVU"
      },
      "id": "zJNJPo8aDKVU"
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------\n",
        "# PositionalEncoding\n",
        "# -------------------------------------------------------------------\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "K1eswhRHCmbC"
      },
      "id": "K1eswhRHCmbC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mask_block Class"
      ],
      "metadata": {
        "id": "yj3OysdGDNT3"
      },
      "id": "yj3OysdGDNT3"
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Mask_block\n",
        "# -------------------------------------------------------------------\n",
        "class Mask_block(nn.Module):\n",
        "    \"\"\"\n",
        "    Learned token mask head with numerically stable logits:\n",
        "      1) Feature -> Linear -> raw logits (per token)\n",
        "      2) Per-sequence standardization: (raw - mean)/std\n",
        "      3) Global learnable scale & bias\n",
        "      4) Sigmoid -> (0,1) mask_out, then zero on pads\n",
        "\n",
        "    Returns:\n",
        "      mask_out: [B,T] in (0,1)\n",
        "      p1:       [B]   sum of mask_out per sequence\n",
        "      p3:       [B]   ((m*(1-m))^2).sum per sequence (peaks at 0.5)\n",
        "      mean_mask: Python float (mask_out mean) for logging\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, weight_matrix, hidden_dim, n_layers,\n",
        "                 max_relative_position=2, pivot=0.5, drop_prob=0.5,\n",
        "                 num_heads=1, reduced_dim=100, temperature=1.0,\n",
        "                 clip_logits: bool = False, clip_value: float = 8.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_layers   = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads  = num_heads\n",
        "        self.pivot      = float(pivot)          # target initial coverage (pre-sigmoid bias)\n",
        "        self.temperature= float(temperature)\n",
        "        self.clip_logits= bool(clip_logits)\n",
        "        self.clip_value = float(clip_value)\n",
        "\n",
        "        # --- feature extractor (keep your existing attention) ---\n",
        "        # Expectation: SelfAttention(..., reduced_dim=reduced_dim) -> [B,T,reduced_dim]\n",
        "        self.attention1 = SelfAttention(emb=hidden_dim, heads=num_heads, reduced_dim=reduced_dim)\n",
        "        self.dropout10  = nn.Dropout(0.1)\n",
        "\n",
        "        # --- NEW: stabilize feature scale going into the head ---\n",
        "        self.mask_norm  = nn.LayerNorm(reduced_dim, elementwise_affine=True)\n",
        "\n",
        "        # --- projection to per-token raw logits (unconstrained) ---\n",
        "        self.mask_head  = nn.Linear(reduced_dim, 1, bias=True)\n",
        "\n",
        "        # --- NEW: learnable global scale & bias applied AFTER per-seq standardization ---\n",
        "        # Start with unit scale and bias so sigmoid(bias) ~= pivot initially.\n",
        "        #self.logit_scale = nn.Parameter(torch.tensor(1.0))                       # scales standardized logits\n",
        "        #self.logit_bias  = nn.Parameter(torch.tensor(_logit(self.pivot)))        # shifts to desired coverage\n",
        "\n",
        "        # Init: keep projection neutral so bias dominates early\n",
        "        with torch.no_grad():\n",
        "            self.mask_head.weight.zero_()\n",
        "            self.mask_head.bias.zero_()  # handled by logit_bias\n",
        "\n",
        "        # misc non-linearities kept (if you reuse them elsewhere)\n",
        "        self.relu      = nn.ReLU()\n",
        "        self.leakyrelu = nn.LeakyReLU()\n",
        "        self.tanh      = nn.Tanh()\n",
        "\n",
        "    def forward(self, embeds, mask_1, digits):\n",
        "        \"\"\"\n",
        "        embeds: [B,T,E]\n",
        "        mask_1: [B,T] bool (True for real tokens)\n",
        "        digits: [B,1] (kept for signature; not used here)\n",
        "        \"\"\"\n",
        "        # 1) features\n",
        "        feats = self.attention1(embeds)          # [B,T,R]\n",
        "        feats = self.dropout10(feats)\n",
        "        feats = self.mask_norm(feats)            # stabilize scale\n",
        "\n",
        "        # 2) raw logits (unbounded), then squeeze channel\n",
        "        logits = self.mask_head(feats).squeeze(-1)  # [B,T]\n",
        "\n",
        "\n",
        "        # 5) sigmoid with optional temperature\n",
        "        if self.temperature != 1.0:\n",
        "            mask = torch.sigmoid(logits / self.temperature)\n",
        "        else:\n",
        "            mask = torch.sigmoid(logits)                         # [B,T] in (0,1)\n",
        "\n",
        "        # 6) zero-out pads but keep gradients\n",
        "        m1f = mask_1.float()\n",
        "        mask_out = mask * m1f                                    # [B,T]\n",
        "\n",
        "        # --- regularizers compatible with your old code ---\n",
        "        # p1: sum activation per sequence (you can divide by length if you prefer)\n",
        "        p1 = mask_out.sum(dim=1)                                 # [B]\n",
        "        # p3: pushes away from 0.5 (max penalty near 0.5)\n",
        "        p3 = ((mask_out * (1.0 - mask_out)) ** 2).sum(dim=1)     # [B]\n",
        "\n",
        "        mean_mask = float(mask_out.mean().detach().item())\n",
        "        return mask_out, p1, p3, mean_mask\n"
      ],
      "metadata": {
        "id": "3cC4fzW7CqZ8"
      },
      "id": "3cC4fzW7CqZ8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment_block"
      ],
      "metadata": {
        "id": "M0FHLKcODR2j"
      },
      "id": "M0FHLKcODR2j"
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Sentiment_block\n",
        "# -------------------------------------------------------------------\n",
        "class Sentiment_block(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, weight_matrix, hidden_dim, n_layers, max_relative_position= 2, pivot=0.5,drop_prob=0.5,num_heads=1, reduced_dim=100):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        embedding_dim = hidden_dim\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.dropout10 = nn.Dropout(0.1)\n",
        "        # linear and sigmoid layers\n",
        "        #self.fc = nn.Linear(embedding_dim, embedding_dim) #regular linear layer\n",
        "        self.fc1 = tiedLinear(reduced_dim, 1) #tied linear layer\n",
        "        self.fc3 = nn.Linear(1 + digits_dim, 1 )\n",
        "        self.pos_enc = PositionalEncoding(d_model = embedding_dim)\n",
        "        self.attention1 = SelfAttention(emb = embedding_dim, heads= self.num_heads, reduced_dim=reduced_dim)\n",
        "        self.attention2 = SelfAttention(emb = embedding_dim, heads= self.num_heads, reduced_dim=reduced_dim)\n",
        "\n",
        "    #    self.pos_attention = Pos_atten_1(hid_dim = embedding_dim, n_heads = self.num_heads )\n",
        "      #  self.re_pos_attention = MultiHeadAttentionLayer(hid_dim = embedding_dim, n_heads = self.num_heads, max_relative_position = max_relative_position )\n",
        "        self.fc2 = tiedLinear(reduced_dim)\n",
        "       # self.fc12 = nn.Linear(embedding_dim, embedding_dim)\n",
        "       # self.fc2 = nn.Linear(embedding_dim, 1)\n",
        "        #self.fc2 = tiedLinear(embedding_dim, embedding_dim)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.leakyrelu = torch.nn.LeakyReLU()\n",
        "        self.sig = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(reduced_dim)\n",
        "\n",
        "    def forward(self, embeds ,mask_1 , mask, digits):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = embeds.shape[0]\n",
        "\n",
        "        embdes = embeds * mask.ge(0.001).unsqueeze(2)\n",
        "     #   author = torch.argmax(digits[:8])\n",
        "        #embeds = self.pos_enc(embeds)\n",
        "       # embeds = self.fc12(embeds)\n",
        "        embeds = self.attention1(embeds)\n",
        "        embeds = self.layer_norm(embeds)\n",
        "        embeds = self.dropout10(embeds)\n",
        "\n",
        "#         embeds = self.attention2(embeds)\n",
        "#         embeds = self.layer_norm(embeds)\n",
        "#         embeds = self.dropout10(embeds)\n",
        "        # pos = (pos*mask_1).int()\n",
        "        # pos = self.pos_embed(pos)\n",
        "        # x3 = self.attention1(pos)\n",
        "      #  x2 = self.pos_attention(embeds, mask_1, pos)\n",
        "\n",
        "\n",
        "     #   out = self.fc2(0*x2 + 1* x3)\n",
        "        save_out0 = embeds\n",
        "        # print('embeddings size:', embeds.size())\n",
        "        out = self.fc2(embeds)\n",
        "        # print('out1 size: ', out.size())\n",
        "        save_out = out\n",
        "        out  = torch.mean(out, 2) # average out embedding dim\n",
        "        # print('output out size: ', out.size())\n",
        "        #out = self.tanh(out/10)\n",
        "        # return last sigmoid output and hidden state\n",
        "\n",
        "        return out, save_out, save_out0\n"
      ],
      "metadata": {
        "id": "KIyCvZpKCuMs"
      },
      "id": "KIyCvZpKCuMs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Synthesizer class"
      ],
      "metadata": {
        "id": "9ouAzjIbDUxP"
      },
      "id": "9ouAzjIbDUxP"
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Synthesizer\n",
        "# -------------------------------------------------------------------\n",
        "class SynthesizerVIvec(nn.Module):\n",
        "    \"\"\"\n",
        "    Bayesian head that consumes a vector sentence embedding [B, S] (+ optional digits).\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_dim, digits_dim=0, prior_sigma=1.0, dropout_p=0.1):\n",
        "        super().__init__()\n",
        "        self.feature_dim = feature_dim\n",
        "        self.digits_dim  = int(digits_dim)\n",
        "        self.dropout     = nn.Dropout(dropout_p)\n",
        "        self.bayes_head  = BayesLinear(feature_dim + self.digits_dim, 1, prior_sigma=prior_sigma)\n",
        "\n",
        "    def forward(self, h_vec, digits=None, sample=True):\n",
        "        # h_vec: [B, S]\n",
        "        x = self.dropout(h_vec)\n",
        "        if digits is not None and self.digits_dim > 0:\n",
        "            x = torch.cat([x, digits], dim=1)  # [B, S + Dg]\n",
        "        logits = self.bayes_head(x, sample=sample).squeeze(-1)  # [B]\n",
        "        probs  = torch.sigmoid(logits)\n",
        "        return probs, logits\n",
        "\n",
        "    def kl(self):\n",
        "        return self.bayes_head.kl()\n",
        "\n",
        "class Synthesizer(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, weight_matrix, hidden_dim, n_layers, max_relative_position= 2, pivot=0.5,drop_prob=0.2,num_heads=1, reduced_dim=100):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        # embedding\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc2 = tiedLinear( hidden_dim)\n",
        "        # linear and sigmoid layers\n",
        "        #self.fc = nn.Linear(embedding_dim, embedding_dim) #regular linear layer\n",
        "        self.fc1 = tiedLinear( hidden_dim, 1) #tied linear layer\n",
        "\n",
        "       # self.pos_enc = PositionalEncoding(d_model = embedding_dim)\n",
        "        self.attention = SelfAttention(emb =  hidden_dim, heads= self.num_heads, reduced_dim=reduced_dim)\n",
        "\n",
        "       # self.re_pos_attention = MultiHeadAttentionLayer(hid_dim = embedding_dim, n_heads = self.num_heads, max_relative_position = max_relative_position )\n",
        "        self.fc2 = tiedLinear( hidden_dim)\n",
        "        self.fc3 = nn.Linear(1 + digits_dim, 1)\n",
        "       # self.fc2 = nn.Linear(embedding_dim, 1)\n",
        "        #self.fc2 = tiedLinear(embedding_dim, embedding_dim)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.leakyrelu = torch.nn.LeakyReLU()\n",
        "        self.sig = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "\n",
        "    def forward(self, sent, digits, mask, use_mask = False):\n",
        "\n",
        "        #p2 = torch.norm(sent, p=2, dim=1)\n",
        "      #  sent=self.tanh(sent)*5\n",
        "        p2 = sent\n",
        "\n",
        "        ###################\n",
        "        '''\n",
        "        for s in sent:\n",
        "            print(s.size())\n",
        "        for m in mask:\n",
        "            print(m.size())\n",
        "        '''\n",
        "        ###################\n",
        "      #  out = out * mask_1\n",
        "        if use_mask == True:\n",
        "            '''\n",
        "            o1 = sent[0] * mask\n",
        "            o2 = sent[1] * mask.unsqueeze(-1)\n",
        "            o3 = sent[2] * mask.unsqueeze(-1)\n",
        "            out = (o1, o2, o3)\n",
        "            '''\n",
        "            out = sent * mask\n",
        "        else:\n",
        "            out = sent\n",
        "\n",
        "        save_out = out\n",
        "     #   out = torch.multiply(out,mask)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        out = torch.mean(out,1)\n",
        "        out = out.reshape((batch_size,1))\n",
        "\n",
        "        #out  = torch.cat((out, digits), dim=1)\n",
        "       # out  = self.fc3(out)\n",
        "        #out = self.tanh(out)\n",
        "        out = torch.mean(out,1)\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "\n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, p2, save_out\n"
      ],
      "metadata": {
        "id": "rrjv2SheCxcE"
      },
      "id": "rrjv2SheCxcE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeds Class"
      ],
      "metadata": {
        "id": "2x5Gb7RHDY2b"
      },
      "id": "2x5Gb7RHDY2b"
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Embeds\n",
        "# -------------------------------------------------------------------\n",
        "class Embeds(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, weight_matrix, hidden_dim):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # embedding False = trainable\n",
        "        self.embedding, num_embeddings, embedding_dim =  create_emb_layer(weight_matrix, False)\n",
        "       # self.embedding = torch.nn.Embedding(vocab_size, hidden_dim)\n",
        "        self.pos_enc = PositionalEncoding(d_model = hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "      #  mask = torch.transpose(mask,0,1)\n",
        "        mask_1 = x.ge(0.1)\n",
        "        #beta\n",
        "       # x = (x * mask).int()\n",
        "        embeds = self.embedding(x)\n",
        "    #    embeds = self.pos_enc(embeds)\n",
        "        # return last sigmoid output and hidden state\n",
        "        return embeds, mask_1"
      ],
      "metadata": {
        "id": "TV3jQMU1C17j"
      },
      "id": "TV3jQMU1C17j",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "UrcFnZ_lssJU"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}